# ACL
## 2017 ACL

[A Constituent-Centric Neural Architecture for Reading Comprehension](http://www.aclweb.org/anthology/P/P17/P17-1129.pdf)

task:reading comprehension ||  data:SQuAD 

[A FOFE-based Local Detection Approach for Named Entity Recognition and Mention Detection](https://arxiv.org/abs/1611.00801)

task:NER、mention detection

[An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge] (https://www.aclweb.org/anthology/P/P17/P17-1021.pdf)

data:WebQuestions || focus：question representation（问题无论它的候选答案是什么，都会被转换成为一个固定长度的vector）

[Attention-over-Attention Neural Networks for Reading Comprehension](https://arxiv.org/abs/1607.04423)

task:阅读理解 



[Automatically Labeled Data Generation for Large Scale Event Extraction](https://www.aclweb.org/anthology/P/P17/P17-1038.pdf)

focus:对event extraction提供标注数据的方法



[Coarse-to-Fine Question Answering for Long Documents ](https://homes.cs.washington.edu/~eunsol/papers/acl17eunsol.pdf)

advantage：高效地扩展到长文档(longer documents)的同时，能够维持甚至提升state-of-the-art模型的性能

[Comparing Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task ](http://cs.rochester.edu/~omidb/papers/apples-apples-semantics.pdf)

task:提出阅读理解新任务GuessTwo（给定一个短段落，与两个真实在语义上相似的(semantically-similar)两个entities分别比较，系统应该能猜出来这两个entities是什么）

[Deep Keyphrase Generation ](https://arxiv.org/abs/1704.06879)

method：使用encoder-decoder框架来预测生成式keypghrase

[Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms](http://ir.ia.ac.cn/bitstream/173211/14522/1/acl2017.pdf)

focus:event detection(引入了identifying和categorizing event) || method:利用argument信息来显式地进行event detection

[Gated Self-Matching Networks for Reading Comprehension and Question Answering](http://www.aclweb.org/anthology/P/P17/P17-1018.pdf)

method:gated self-matching networks || data:SQuAD

[Gated-Attention Readers for Text Comprehension](https://arxiv.org/abs/1606.01549)

data:document上回答cloze风格 || method:multi-hop架构和一个新的attention机制结合

[Generating Natural Answer by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning](http://www.nlpr.ia.ac.cn/cip/shizhuhe/articles/acl2017-coreqa.pdf)

data:kbqa








